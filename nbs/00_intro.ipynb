{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868b4d3c",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: index.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719becee-125a-4c3a-85ce-6f983fa8c9c6",
   "metadata": {},
   "source": [
    "# HAMUX\n",
    "> **HAMUX** (**H**ierarchical **A**ssociative **M**emory **U**ser e**X**perience) is a Deep Learning framework designed around *energy*. Every architecture built in HAMUX is a global, Lyapunov energy function. HAMUX bridges modern AI architectures and Hopfield Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6b249-3cbc-436d-95d6-18beca4830d1",
   "metadata": {},
   "source": [
    "## What is HAMUX?\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/HyperSynapse-fig1.png\" alt=\"HAMUX Overview\" width=\"700\"/>\n",
    "<figcaption style=\"color:#999\">Explaining the \"energy fundamentals\" of HAMUX (Layers and Synapses, left) using a 4-layer, 3-synapse example HAM (middle) that can be built using the pseudocode on the right. (NOTE: code is not runnable in newer versions of HAMUX as the API has changed).</figcaption>\n",
    "</figure>\n",
    "\n",
    "HAMUX defines two fundamental building blocks of energy: the [**üåÄneuron layer**](https://bhoov.github.io/hamux/neurons.html) and the [**ü§ùhypersynapse**](https://bhoov.github.io/hamux/synapses.html) (an abstraction of a pairwise synapse\n",
    "to include many-body interactions) connected via a\n",
    "[**hypergraph**](https://en.wikipedia.org/wiki/Hypergraph). \n",
    "It is a fully dynamical system, where the ‚Äúhidden state‚Äù $x_i^\\ell$ of each layer\n",
    "$\\ell$ (blue squares in the figure below) is an independent variable that\n",
    "evolves over time. The update rule of each layer is entirely local: neurons evolve deterministically by accumulating \"signals\" from only the **connected synapses** (i.e., the red circles in the figure below). This is shown in the\n",
    "following equation:\n",
    "\n",
    "$$\\tau \\frac{d x_{i}^{\\ell}}{dt} = -\\frac{\\partial E}{\\partial g_i^\\ell}$$\n",
    "\n",
    "where $g_i^\\ell$ are the *activations* (i.e., non-linearities) on each\n",
    "neuron layer $\\ell$, described in the section on [Neuron\n",
    "Layers](#üåÄNeuron-Layers). Concretely, we implement the above\n",
    "differential equation as the following discretized equation (where the\n",
    "bold ${\\mathbf x}_\\ell$ is the collection of all elements in layer $\\ell$‚Äôs\n",
    "state):\n",
    "\n",
    "$$ \\mathbf{x}_\\ell^{(t+1)} = \\mathbf{x}_\\ell^{(t)} - \\frac{dt}{\\tau} \\nabla_{\\mathbf{g}_\\ell}E(t)$$\n",
    "\n",
    "HAMUX handles all the complexity of scaling this fundamental update\n",
    "equation to many üåÄneurons and ü§ùhypersynapses with as minimal overhead as possible. Essentially, HAMUX is a simplified hypergraph library that allows us to modularly compose energy functions. HAMUX makes it easy to:\n",
    "\n",
    "1.  Inject your data into the associative memory\n",
    "2.  Perform inference (a.k.a., \"Memory Retrieval\", \"Error correction\", or \"the forward pass\") by **autograd**-computed gradient descent of the energy function!\n",
    "3. Build complex, powerful networks using arbitrary energy functions. E.g., we can easily build the [Energy Transformer](https://arxiv.org/abs/2302.07253) in this framework using a couple lines of code. See [this tutorial](https://bhoov.github.io/hamux/tutorials/energy_transformer.html) (WIP).\n",
    "\n",
    "\n",
    "We are continually trying to enrich our [`tutorials`](https://bhoov.github.io/hamux/tutorials/), which are implemented as working Jupyter Notebooks. HAMUX is built on the amazing [JAX](https://github.com/google/jax) and [`equinox`](https://github.com/patrick-kidger/equinox) libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64717aac-b6b1-484c-9f78-b7e10a1a80d3",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "\n",
    "We can build a simple 4 layer HAM architecture using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21623a2-d47f-4688-9f73-422cb058b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bbhamux as hmx\n",
    "# from hamux.lagrangians import lagr_identity, lagr_sigmoid, lagr_softmax, lagr_tanh\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7567ce-1d28-4b8d-9acf-10c8ef180cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (32,32,3) # E.g., CIFAR images\n",
    "                           \n",
    "neurons = {\n",
    "    \"image\": hmx.Neurons(hmx.lagr_identity, img_shape),\n",
    "    \"patch\": hmx.Neurons(hmx.lagr_tanh, (11,11,16)),\n",
    "    \"label\": hmx.Neurons(hmx.lagr_softmax, (10,)),\n",
    "    \"memory\": hmx.Neurons(hmx.lagr_softmax, (25,))\n",
    "}\n",
    "\n",
    "rng = jr.PRNGKey(0)\n",
    "k1, k2, k3, rng = jr.split(rng, 4)\n",
    "\n",
    "synapses = {\n",
    "    \"conv1\": hmx.ConvSynapse(k1, 16,3, (3,3), window_strides=(3,3)),\n",
    "    \"dense1\": hmx.DenseSynapse(k2, 10, 25),\n",
    "    \"dense2\": hmx.DenseSynapse(),\n",
    "}\n",
    "\n",
    "connections = [\n",
    "    ([\"image\",\"patch\"], \"conv1\"),\n",
    "    ([\"label\", \"memory\"], \"dense1\"),\n",
    "    ([\"\", \"\"], \"dense2\"),\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-mine2-py",
   "language": "python",
   "name": "conda-env-mine2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
