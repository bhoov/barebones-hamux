{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868b4d3c",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: index.html\n",
    "title: HAMUX\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719becee-125a-4c3a-85ce-6f983fa8c9c6",
   "metadata": {},
   "source": [
    "> **HAMUX** (**H**ierarchical **A**ssociative **M**emory **U**ser e**X**perience) is a Deep Learning framework designed around *energy*. Every architecture built in HAMUX is a global, Lyapunov energy function. HAMUX bridges modern AI architectures and Hopfield Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6b249-3cbc-436d-95d6-18beca4830d1",
   "metadata": {},
   "source": [
    "## What is HAMUX?\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/HyperSynapse-fig1.png\" alt=\"HAMUX Overview\" width=\"700\"/>\n",
    "<figcaption style=\"color:#999\">Explaining the \"energy fundamentals\" of HAMUX (Layers and Synapses, left) using a 4-layer, 3-synapse example HAM (middle) that can be built using the pseudocode on the right. (NOTE: code is not runnable in newer versions of HAMUX as the API has changed).</figcaption>\n",
    "</figure>\n",
    "\n",
    "HAMUX defines two fundamental building blocks of energy: the [**ðŸŒ€neuron layer**](https://bhoov.github.io/hamux/neurons.html) and the [**ðŸ¤hypersynapse**](https://bhoov.github.io/hamux/synapses.html) (an abstraction of a pairwise synapse\n",
    "to include many-body interactions) connected via a\n",
    "[**hypergraph**](https://en.wikipedia.org/wiki/Hypergraph). \n",
    "It is a fully dynamical system, where the â€œhidden stateâ€ $x_i^\\ell$ of each layer\n",
    "$\\ell$ (blue squares in the figure below) is an independent variable that\n",
    "evolves over time. The update rule of each layer is entirely local: neurons evolve deterministically by accumulating \"signals\" from only the **connected synapses** (i.e., the red circles in the figure below). This is shown in the\n",
    "following equation:\n",
    "\n",
    "$$\\tau \\frac{d x_{i}^{\\ell}}{dt} = -\\frac{\\partial E}{\\partial g_i^\\ell}$$\n",
    "\n",
    "where $g_i^\\ell$ are the *activations* (i.e., non-linearities) on each\n",
    "neuron layer $\\ell$, described in the section on [Neuron\n",
    "Layers](#ðŸŒ€Neuron-Layers). Concretely, we implement the above\n",
    "differential equation as the following discretized equation (where the\n",
    "bold ${\\mathbf x}_\\ell$ is the collection of all elements in layer $\\ell$â€™s\n",
    "state):\n",
    "\n",
    "$$ \\mathbf{x}_\\ell^{(t+1)} = \\mathbf{x}_\\ell^{(t)} - \\frac{dt}{\\tau} \\nabla_{\\mathbf{g}_\\ell}E(t)$$\n",
    "\n",
    "HAMUX handles all the complexity of scaling this fundamental update\n",
    "equation to many ðŸŒ€neurons and ðŸ¤hypersynapses with as minimal overhead as possible. Essentially, HAMUX is a simplified hypergraph library that allows us to modularly compose energy functions. HAMUX makes it easy to:\n",
    "\n",
    "1.  Inject your data into the associative memory\n",
    "2.  Perform inference (a.k.a., \"Memory Retrieval\", \"Error correction\", or \"the forward pass\") by **autograd**-computed gradient descent of the energy function!\n",
    "3. Build complex, powerful networks using arbitrary energy functions. E.g., we can easily build the [Energy Transformer](https://arxiv.org/abs/2302.07253) in this framework using a couple lines of code. See [this tutorial](https://bhoov.github.io/hamux/tutorials/energy_transformer.html) (WIP).\n",
    "\n",
    "\n",
    "We are continually trying to enrich our [`tutorials`](https://bhoov.github.io/hamux/tutorials/), which are implemented as working Jupyter Notebooks. HAMUX is built on the amazing [JAX](https://github.com/google/jax) and [`equinox`](https://github.com/patrick-kidger/equinox) libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64717aac-b6b1-484c-9f78-b7e10a1a80d3",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "\n",
    "We can build a simple 4 layer HAM architecture using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21623a2-d47f-4688-9f73-422cb058b3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hamux as hmx\n",
    "from hamux.lagrangians import lagr_identity, lagr_sigmoid, lagr_softmax, lagr_tanh\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7567ce-1d28-4b8d-9acf-10c8ef180cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (32,32,3) # E.g., CIFAR images\n",
    "\n",
    "class ConvSynapse(Synapse):\n",
    "    conv: tx.Module    \n",
    "    \n",
    "    def __init__(self, conv:tx.Module):\n",
    "        self.conv = conv\n",
    "\n",
    "    def energy(self, g1, g2):\n",
    "        if self.initializing():\n",
    "            key = tx.next_key()\n",
    "            features_in = g1.shape[0]\n",
    "            features_out = g2.shape[0]\n",
    "            self.conv = self.conv.init(key, g1)\n",
    "        return jnp.multiply(g2, self.conv(g1)).sum()\n",
    "    \n",
    "    \n",
    "def DenseSynapse(eqx.Module):\n",
    "    W: jax.Array\n",
    "    \n",
    "    def __init__(key: jax.Array, \n",
    "                 dim1:int, # Dimension of input `g1` to the energy\n",
    "                 dim2:int, # Dimension of input `g2` to the energy\n",
    "                ):\n",
    "        self.W = 0.01 * jr.normal(key, (dim1, dim2))\n",
    "        \n",
    "    def __call__(self, g1:jax.Array, g2:jax.Array):\n",
    "        return -jnp.einsum(\"...c,...d,cd->\", g1, g2, self.W)\n",
    "    \n",
    "def ConvSynapse(eqx.Module):\n",
    "    W: jax.Array\n",
    "    \n",
    "    def __init__(key: jax.Array, \n",
    "                 dim1:int, # Dimension of input `g1` to the energy\n",
    "                 dim2:int, # Dimension of input `g2` to the energy\n",
    "                ):\n",
    "        self.W = 0.01 * jr.normal(key, (dim1, dim2))\n",
    "        \n",
    "    def __call__(self, g1:jax.Array, g2:jax.Array):\n",
    "        return -jnp.einsum(\"...c,...d,cd->\", g1, g2, self.W)\n",
    "                           \n",
    "neurons = {\n",
    "    \"image\": hmx.Neurons(lagr_identity, img_shape),\n",
    "    \"patch\": hmx.Neurons(lagr_tanh, (11,11,16)),\n",
    "    \"label\": hmx.Neurons(lagr_softmax, (10,)),\n",
    "    \"memory\": hmx.Neurons(lagr_softmax, (25,))\n",
    "}\n",
    "\n",
    "synapses = {\n",
    "    \"conv1\": hmx.ConvSynapse((3,3), strides=3),\n",
    "    \"dense1\": hmx.DenseSynapse(),\n",
    "    \"dense2\": hmx.DenseSynapse(),\n",
    "}\n",
    "\n",
    "connections = [\n",
    "    ([\"image\",\"patch\"], \"conv1\"),\n",
    "    ([\"label\", \"memory\"], \"dense1\"),\n",
    "    ([\"\", \"\"], \"dense2\"),\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
