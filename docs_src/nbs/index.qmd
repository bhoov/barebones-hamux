---
title: HAMUX
subtitle: A new class of deep learning library built around **ENERGY**
---

<img src="https://raw.githubusercontent.com/bhoov/hamux/main/assets/header.png" alt="HAMUX logo" width="400"/>

```{python}
#| echo: false
from bbhamux import *
```

> `bbhamux` is a minimal version of [`hamux`](https://github.com/bhoov/hamux) to construct Hierarchical Associative Memories (i.e., super-powered Hopfield Networks). It represents research code that is designed to be easy to use, easy to hack.

## Getting started

::: callout-note
This documentation walks through how to use the package as if installing from `pip`, but the main logic for this repo lives in a **single `.py` file** (`src/bbhamux/bbhamux.py`). See [the original README](https://github.com/bhoov/barebones-hamux) for instructions on how to just copy and use the important file.
:::

``` bash
pip install bbhamux
```

## HAMUX is a universal abstraction for Hopfield Networks

HAMUX fully captures the the energy fundamentals of Hopfield Networks and enables anyone to:

-   üß† Build **DEEP** Hopfield nets

-   üß± With modular **ENERGY** components

-   üèÜ That resemble modern DL operations

**Every** architecture built using HAMUX is a *dynamical system* guaranteed to have a *tractable energy* function that *converges* to a fixed point. Our deep [Hierarchical Associative Memories](https://arxiv.org/abs/2107.06446) (HAMs) have several additional advantages over traditional [Hopfield Networks](http://www.scholarpedia.org/article/Hopfield_network) (HNs):

| Hopfield Networks (HNs) | Hierarchical Associative Memories (HAMs) |
|----|----|
| HNs are only **two layers** systems | HAMs connect **any number** of layers |
| HNs model only **simple relationships** between layers | HAMs model **any complex but differentiable operation** (e.g., convolutions, pooling, attention, $\ldots$) |
| HNs use only **pairwise synapses** | HAMs use **many-body synapses** (which we denote **HyperSynapses**) |

## How does HAMUX work?

> **HAMUX** is a [hypergraph](%60https://en.wikipedia.org/wiki/Hypergraph)\` of üåÄneurons connected via ü§ùhypersynapses, an abstraction sufficiently general to model the complexity of connections used in modern AI architectures.

HAMUX defines two fundamental building blocks of energy: the **üåÄneuron layer** and the **ü§ùhypersynapse** (an abstraction of a pairwise synapse to include many-body interactions) connected via a [**hypergraph**](https://en.wikipedia.org/wiki/Hypergraph). It is a fully dynamical system, where the "hidden state" $x_i^l$ of each layer $l$ (blue squares in the figure below) is an independent variable that evolves over time. The update rule of each layer is entirely local; only signals from a layer's connected synapses (red circles in the figure below) can tell the hidden state how to change. This is shown in the following equation:

$$\tau \frac{d x_{i}^{l}}{dt} = -\frac{\partial E}{\partial g_i^l}$$

where $g_i^l$ are the *activations* (i.e., non-linearities) on each neuron layer, described in the section on [Neuron Layers](#üåÄNeuron-Layers). Concretely, we implement the above differential equation as the following discretized equation (where the bold ${\mathbf x}_l$ is the collection of all elements in layer $l$'s state):

$$ \mathbf{x}_l^{(t+1)} = \mathbf{x}_l^{(t)} - \frac{dt}{\tau} \nabla_{\mathbf{g}_l}E(t)$$

HAMUX handles all the complexity of scaling this fundamental update equation to many layers and hyper synapses. In addition, it provides a *framework* to:

1.  Implement your favorite Deep Learning operations as a [HyperSynapse](https://bhoov.github.io/hamux/synapses.html)
2.  Port over your favorite activation functions as [Lagrangians](https://bhoov.github.io/hamux/lagrangians.html)
3.  Connect your layers and hypersynapses into a [HAM](https://bhoov.github.io/hamux/ham.html) (using a hypergraph as the data structure)
4.  Inject your data into the associative memory
5.  Automatically calculate and descend the energy given the hidden states at any point in time

Use these features to train any hierarchical associative memory on your own data! All of this made possible by [JAX](https://github.com/google/jax).

The `examples/` subdirectory contains a (growing) list of examples on how to apply HAMUX on real data.

## Contributing to the docs

```         
cd docs_src/
nbdev_preview # Live preview docs site
nbdev_test # Test the code examples in the docs
nbdev_docs # Deploy the static docs site
```